{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6361fff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "53a4ab6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms as T\n",
    "\n",
    "class HW_Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data_root, transform=None, max_seq_len=50):\n",
    "        self.data_root = data_root\n",
    "        self.transform = transform\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        self.samples = self.load_dataset()\n",
    "        self.char_to_int = self.build_vocab()\n",
    "        \n",
    "    def load_dataset(self):\n",
    "        labelled_pairs = []\n",
    "        with open(f\"{self.data_root}/train.txt\", 'r', encoding='utf-8') as file:\n",
    "            file.seek(0)\n",
    "            lines = file.readlines()\n",
    "            for line in lines:\n",
    "                line = line.split()\n",
    "                labelled_pairs.append((line[0], line[1]))\n",
    "        return labelled_pairs\n",
    "    \n",
    "    def build_vocab(self):\n",
    "        unique_chars = []\n",
    "        with open(f\"{self.data_root}/hindi_vocab.txt\", 'r', encoding='utf-8') as file:\n",
    "            file.seek(0)\n",
    "            lines = file.readlines()\n",
    "            for line in lines:\n",
    "                for char in line:\n",
    "                    if char not in unique_chars:\n",
    "                        unique_chars.append(char)\n",
    "        char_to_int = {'<PAD>': 0, '<UNK>': 1}\n",
    "        for i in range(len(unique_chars)):\n",
    "            char_to_int[unique_chars[i]] = i + 2\n",
    "        return char_to_int\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, text = self.samples[idx]\n",
    "        \n",
    "        # load image\n",
    "        img = Image.open(os.path.join(self.data_root, img_path)).convert('L')\n",
    "        if self.transform:\n",
    "            img_tensor = self.transform(img)\n",
    "        \n",
    "        # Process text\n",
    "        text_ids = [self.char_to_int.get(c, self.char_to_int['<UNK>']) for c in text]\n",
    "        padded_text_ids = torch.zeros(self.max_seq_len, dtype=torch.long)\n",
    "        padded_text_ids[:len(text_ids)] = torch.tensor(text_ids)\n",
    "        \n",
    "        return img_tensor, padded_text_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7c73fbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # Use an LSTM to process the sequence of characters\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, text_ids):\n",
    "        # text_ids shape: (Batch_size, Max_seq_len)\n",
    "        embedded = self.embedding(text_ids)\n",
    "        # Pass through RNN/LSTM\n",
    "        _, (hidden, _) = self.rnn(embedded)\n",
    "        # Use the final hidden state and project to the desired output size\n",
    "        # hidden shape: (1, Batch_size, Hidden_size)\n",
    "        condition = self.fc(hidden.squeeze(0))\n",
    "        return condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "50fb8bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, condition_dim, img_channels, img_size_h, img_size_w):\n",
    "        super().__init__()\n",
    "        self.img_size_h = img_size_h\n",
    "        self.img_size_w = img_size_w\n",
    "        self.img_channels = img_channels\n",
    "        \n",
    "        # We start by projecting and reshaping the combined input\n",
    "        # Input: z_dim + condition_dim\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(z_dim + condition_dim, 1024 * (img_size_h // 16) * (img_size_w // 16)),\n",
    "            nn.BatchNorm1d(1024 * (img_size_h // 16) * (img_size_w // 16)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Now, we upsample using ConvTranspose2d\n",
    "        # We'll go from (4x16) -> (8x32) -> (16x64) -> (32x128) -> (64x256)\n",
    "        self.gen = nn.Sequential(\n",
    "            # Input: (1024, 4, 16)\n",
    "            self._block(1024, 512, 4, 2, 1),  # -> (512, 8, 32)\n",
    "            self._block(512, 256, 4, 2, 1),   # -> (256, 16, 64)\n",
    "            self._block(256, 128, 4, 2, 1),   # -> (128, 32, 128)\n",
    "            \n",
    "            # Final layer to get to the target size and channels\n",
    "            nn.ConvTranspose2d(\n",
    "                128, img_channels, kernel_size=4, stride=2, padding=1\n",
    "            ),\n",
    "            # Output: (img_channels, 64, 256)\n",
    "            nn.Tanh() # Normalize output to [-1, 1], matching data normalization\n",
    "        )\n",
    "\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels, out_channels, kernel_size, stride, padding, bias=False\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, z, condition):\n",
    "        # z shape: (N, z_dim)\n",
    "        # condition shape: (N, condition_dim)\n",
    "        \n",
    "        # Combine noise and condition\n",
    "        combined_input = torch.cat([z, condition], dim=1) # (N, z_dim + condition_dim)\n",
    "        \n",
    "        # Project and reshape\n",
    "        x = self.fc(combined_input)\n",
    "        # Reshape to (N, 1024, H/16, W/16) -> (N, 1024, 4, 16)\n",
    "        x = x.view(-1, 1024, self.img_size_h // 16, self.img_size_w // 16)\n",
    "        \n",
    "        # Pass through the generator blocks\n",
    "        return self.gen(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "9f436eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, condition_dim, img_channels, img_size_h, img_size_w):\n",
    "        super().__init__()\n",
    "        \n",
    "        # CNN blocks to process the image\n",
    "        # Input: (img_channels, 64, 256)\n",
    "        self.disc = nn.Sequential(\n",
    "            # -> (128, 32, 128)\n",
    "            self._block(img_channels, 128, 4, 2, 1, use_norm=False), \n",
    "            # -> (256, 16, 64)\n",
    "            self._block(128, 256, 4, 2, 1),\n",
    "            # -> (512, 8, 32)\n",
    "            self._block(256, 512, 4, 2, 1),\n",
    "            # -> (1024, 4, 16)\n",
    "            self._block(512, 1024, 4, 2, 1),\n",
    "        )\n",
    "        \n",
    "        # Flatten and combine with condition\n",
    "        # Output of disc: (N, 1024, 4, 16)\n",
    "        # Flattened size: 1024 * 4 * 16 = 65536\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(1024 * (img_size_h // 16) * (img_size_w // 16) + condition_dim, 1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(1024, 1)\n",
    "            # No Sigmoid here! We'll use BCEWithLogitsLoss for stability.\n",
    "        )\n",
    "\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding, use_norm=True):\n",
    "        layers = [\n",
    "            nn.Conv2d(\n",
    "                in_channels, out_channels, kernel_size, stride, padding, bias=False\n",
    "            )\n",
    "        ]\n",
    "        if use_norm:\n",
    "            layers.append(nn.BatchNorm2d(out_channels))\n",
    "        layers.append(nn.LeakyReLU(0.2))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, condition):\n",
    "        # x shape: (N, C, H, W)\n",
    "        # condition shape: (N, condition_dim)\n",
    "        \n",
    "        x = self.disc(x) # (N, 1024, 4, 16)\n",
    "        \n",
    "        # Flatten and concatenate condition\n",
    "        x_flat = x.view(x.shape[0], -1) # (N, 65536)\n",
    "        combined = torch.cat([x_flat, condition], dim=1) # (N, 65536 + condition_dim)\n",
    "        \n",
    "        # Classify\n",
    "        return self.fc(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "5f900b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchvision.transforms import functional as F\n",
    "\n",
    "class FixedHeightResize:\n",
    "    \"\"\"Resizes an image to a fixed height while preserving the aspect ratio.\"\"\"\n",
    "    def __init__(self, target_height=64):\n",
    "        self.target_height = target_height\n",
    "\n",
    "    def __call__(self, img):\n",
    "        # Get original dimensions (PIL image returns width, height)\n",
    "        original_width, original_height = img.size\n",
    "        # Calculate the new width to maintain the aspect ratio\n",
    "        aspect_ratio = original_width / original_height\n",
    "        new_width = math.ceil(aspect_ratio * self.target_height)\n",
    "        # Resize the image\n",
    "        resized_img = img.resize((new_width, self.target_height), Image.BICUBIC)\n",
    "        return resized_img\n",
    "\n",
    "class PadToWidth:\n",
    "    \"\"\"Pads an image (PIL or Tensor) to a fixed width while maintaining height.\"\"\"\n",
    "    def __init__(self, target_width, fill_color=255): \n",
    "        # For standard handwriting data on a white background, 255 (white) is best.\n",
    "        self.target_width = target_width\n",
    "        self.fill_color = fill_color \n",
    "\n",
    "    def __call__(self, img):\n",
    "        if not isinstance(img, Image.Image):\n",
    "             # Assumes we are operating on a PIL Image before ToTensor()\n",
    "             raise TypeError(\"Input must be a PIL Image.\")\n",
    "\n",
    "        current_width = img.width\n",
    "        \n",
    "        if current_width >= self.target_width:\n",
    "            # If the image is already wide enough (or too wide), we just center-crop it.\n",
    "            # This handles outliers, though you should choose max_width carefully.\n",
    "            return F.center_crop(img, (img.height, self.target_width)) \n",
    "        \n",
    "        # Calculate padding needed (only on the right)\n",
    "        padding_needed = self.target_width - current_width\n",
    "        \n",
    "        # Pad with the background color (left, top, right, bottom)\n",
    "        # We only pad on the right (right padding = padding_needed)\n",
    "        padding = (0, 0, padding_needed, 0) \n",
    "        \n",
    "        return F.pad(img, padding, fill=self.fill_color)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311a858f",
   "metadata": {},
   "source": [
    "### Testing for the time being\n",
    "Move to another notebook once all looks good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "44c000af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu\n",
      "Starting Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                     | 14/69853 [00:52<73:28:06,  3.79s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[162], line 109\u001b[0m\n\u001b[1;32m    107\u001b[0m d_fake_output \u001b[38;5;241m=\u001b[39m disc(fake_images\u001b[38;5;241m.\u001b[39mdetach(), condition\u001b[38;5;241m.\u001b[39mdetach())\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    108\u001b[0m d_fake_loss \u001b[38;5;241m=\u001b[39m criterion(d_fake_output, torch\u001b[38;5;241m.\u001b[39mzeros_like(d_fake_output))\n\u001b[0;32m--> 109\u001b[0m d_fake_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# Total discriminator loss\u001b[39;00m\n\u001b[1;32m    112\u001b[0m d_loss \u001b[38;5;241m=\u001b[39m (d_real_loss \u001b[38;5;241m+\u001b[39m d_fake_loss) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf/lib/python3.11/site-packages/torch/_tensor.py:647\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    639\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    640\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    645\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    646\u001b[0m     )\n\u001b[0;32m--> 647\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    648\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    649\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf/lib/python3.11/site-packages/torch/autograd/__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m _engine_run_backward(\n\u001b[1;32m    355\u001b[0m     tensors,\n\u001b[1;32m    356\u001b[0m     grad_tensors_,\n\u001b[1;32m    357\u001b[0m     retain_graph,\n\u001b[1;32m    358\u001b[0m     create_graph,\n\u001b[1;32m    359\u001b[0m     inputs_tuple,\n\u001b[1;32m    360\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    361\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    362\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf/lib/python3.11/site-packages/torch/autograd/graph.py:829\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    830\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    831\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    833\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torchvision.utils as vutils\n",
    "from tqdm import tqdm\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "    \n",
    "print(f\"Using {DEVICE}\")\n",
    "\n",
    "# Data setup\n",
    "IMG_H = 64\n",
    "IMG_W = 256\n",
    "IMG_CHANNELS = 1\n",
    "MAX_SEQ_LEN = 50\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Model params\n",
    "VOCAB_SIZE = len(dataset.char_to_int)\n",
    "EMBEDDING_DIM = 256\n",
    "CONDITION_DIM = 128 # The output size of TextEncoder\n",
    "Z_DIM = 100 # Noise dimension\n",
    "LR = 2e-4\n",
    "BETA1 = 0.5\n",
    "NUM_EPOCHS = 100 # Add more later\n",
    "\n",
    "data_transforms = T.Compose([\n",
    "    FixedHeightResize(target_height=64),\n",
    "    PadToWidth(target_width=256),\n",
    "    T.Grayscale(num_output_channels=1),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "dataset = HW_Dataset(data_root='IIIT-HW-Hindi_v1', transform=data_transforms)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "# Initialize Models\n",
    "text_encoder = TextEncoder(VOCAB_SIZE, EMBEDDING_DIM, CONDITION_DIM, CONDITION_DIM).to(DEVICE)\n",
    "gen = Generator(Z_DIM, CONDITION_DIM, IMG_CHANNELS, IMG_H, IMG_W).to(DEVICE)\n",
    "disc = Discriminator(CONDITION_DIM, IMG_CHANNELS, IMG_H, IMG_W).to(DEVICE)\n",
    "\n",
    "# Initialize weights\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "gen.apply(weights_init)\n",
    "disc.apply(weights_init)\n",
    "text_encoder.apply(weights_init)\n",
    "\n",
    "# Loss and Optimizers\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "opt_disc = optim.Adam(disc.parameters(), lr=LR, betas=(BETA1, 0.999))\n",
    "opt_gen = optim.Adam(\n",
    "    list(gen.parameters()) + list(text_encoder.parameters()), \n",
    "    lr=LR, \n",
    "    betas=(BETA1, 0.999)\n",
    ")\n",
    "\n",
    "# We'll use a fixed batch of noise and text to see G's progress\n",
    "fixed_noise = torch.randn(BATCH_SIZE, Z_DIM).to(DEVICE)\n",
    "fixed_batch_data = next(iter(train_loader))\n",
    "fixed_real_images, fixed_text_ids = fixed_batch_data\n",
    "fixed_text_ids = fixed_text_ids.to(DEVICE)\n",
    "fixed_real_images = fixed_real_images.to(DEVICE)\n",
    "\n",
    "if not os.path.exists(\"outputs\"):\n",
    "    os.makedirs(\"outputs\")\n",
    "\n",
    "# Save the fixed real batch for comparison\n",
    "vutils.save_image(fixed_real_images, \"outputs/real_samples.png\", normalize=True)\n",
    "\n",
    "print(\"Starting Training...\")\n",
    "\n",
    "progress_bar = tqdm(total=dataset.__len__())\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for batch_idx, (real_images, text_ids) in enumerate(train_loader):\n",
    "        real_images = real_images.to(DEVICE)\n",
    "        text_ids = text_ids.to(DEVICE)\n",
    "        \n",
    "        # Get the condition vector (c)\n",
    "        condition = text_encoder(text_ids)\n",
    "        \n",
    "        # --- Train Discriminator ---\n",
    "        opt_disc.zero_grad()\n",
    "        \n",
    "        # Train with real images\n",
    "        # Use condition.detach() so text encoder is not updated\n",
    "        d_real_output = disc(real_images, condition.detach()).reshape(-1)\n",
    "        d_real_loss = criterion(d_real_output, torch.ones_like(d_real_output))\n",
    "        d_real_loss.backward()\n",
    "        \n",
    "        # Train with fake images\n",
    "        noise = torch.randn(real_images.size(0), Z_DIM).to(DEVICE)\n",
    "        fake_images = gen(noise, condition.detach())\n",
    "        # Use condition.detach() so generator is not updated\n",
    "        d_fake_output = disc(fake_images.detach(), condition.detach()).reshape(-1)\n",
    "        d_fake_loss = criterion(d_fake_output, torch.zeros_like(d_fake_output))\n",
    "        d_fake_loss.backward()\n",
    "        \n",
    "        # Total discriminator loss\n",
    "        d_loss = (d_real_loss + d_fake_loss) / 2\n",
    "        opt_disc.step()\n",
    "        \n",
    "        # --- Train Generator ---\n",
    "        opt_gen.zero_grad()\n",
    "        \n",
    "        # Get fresh condition vector (no detaching so encoder is updated)\n",
    "        condition_gen = text_encoder(text_ids)\n",
    "        \n",
    "        # Generate new fake images\n",
    "        fake_images_gen = gen(noise, condition_gen)\n",
    "        \n",
    "        # See what the discriminator thinks (no detaching)\n",
    "        g_output = disc(fake_images_gen, condition_gen).reshape(-1)\n",
    "        \n",
    "        # Calculate loss (Generator wants discriminator to think they are real)\n",
    "        g_loss = criterion(g_output, torch.ones_like(g_output))\n",
    "        \n",
    "        # Backprop (updates both generatir and text encoder)\n",
    "        g_loss.backward()\n",
    "        opt_gen.step()\n",
    "        \n",
    "        # Logging\n",
    "        if (batch_idx + 1) % 100 == 0:\n",
    "            print(\n",
    "                f\"[Epoch {epoch+1}/{NUM_EPOCHS}] [Batch {batch_idx+1}/{len(train_loader)}] \"\n",
    "                f\"D Loss: {d_loss.item():.4f} | G Loss: {g_loss.item():.4f}\"\n",
    "            )\n",
    "        progress_bar.update(1)\n",
    "            \n",
    "    # Save generated images at the end of each epoch\n",
    "    with torch.no_grad():\n",
    "        fixed_condition = text_encoder(fixed_text_ids)\n",
    "        fake_samples = gen(fixed_noise, fixed_condition)\n",
    "        vutils.save_image(\n",
    "            fake_samples,\n",
    "            f\"outputs/fake_samples_epoch_{epoch+1}.png\",\n",
    "            normalize=True\n",
    "        )\n",
    "\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab7a981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n",
      "Loading models and optimizers from epoch 38...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shell\\PycharmProjects\\WAICY25-26\\training.py:102: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All states loaded successfully. Resuming training from batch 1599\n",
      "Starting Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/135930 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "%run training.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7289ea0b7fe8380",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_funcs import *\n",
    "\n",
    "output = 'सलाम'\n",
    "char_to_int = pd.read_csv(\"char_to_int.csv\")\n",
    "char_to_int = char_to_int['0'].to_dict()\n",
    "char_to_int = {value: key for key, value in char_to_int.items()}\n",
    "text_ids = [char_to_int.get(c, char_to_int['<UNK>']) for c in output]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if torch.cuda.is_available():\n",
    "        DEVICE = \"cuda\"\n",
    "    else:\n",
    "        DEVICE = \"cpu\"\n",
    "\n",
    "    VOCAB_SIZE = len(char_to_int)\n",
    "    EMBEDDING_DIM = 256\n",
    "    CONDITION_DIM = 128\n",
    "    Z_DIM = 100\n",
    "    IMG_H = 64\n",
    "    IMG_W = 256\n",
    "    IMG_CHANNELS = 1\n",
    "    MAX_SEQ_LEN = 50\n",
    "\n",
    "    text_encoder = TextEncoder(VOCAB_SIZE, EMBEDDING_DIM, CONDITION_DIM, CONDITION_DIM).to(DEVICE)\n",
    "    gen = Generator(Z_DIM, CONDITION_DIM, IMG_CHANNELS, IMG_H, IMG_W).to(DEVICE)\n",
    "\n",
    "    checkpoints = glob.glob(\"checkpoints/checkpoint_epoch_*.pth\")\n",
    "    latest_epoch_num = max([int(f.split('_')[-1].split('.')[0]) for f in checkpoints])\n",
    "    checkpoint_path = f\"checkpoints/checkpoint_epoch_{latest_epoch_num}.pth\"\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n",
    "    gen.load_state_dict(checkpoint['gen_state_dict'])\n",
    "    text_encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "    gen.eval()\n",
    "    text_encoder.eval()\n",
    "    \n",
    "    noise = torch.randn(1, Z_DIM, device=DEVICE)\n",
    "    text_tensor = torch.tensor(text_ids, dtype=torch.long, device=DEVICE).unsqueeze(0)\n",
    "    condition = text_encoder(text_tensor)\n",
    "    fake_sample = gen(noise, condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61ffad9-38ac-47a9-881c-c4066c33c4cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de7b4dd-bc7e-4127-b402-dac2e51db33e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
