{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6361fff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "53a4ab6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms as T\n",
    "\n",
    "class HW_Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data_root, transform=None, max_seq_len=50):\n",
    "        self.data_root = data_root\n",
    "        self.transform = transform\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        self.samples = self.load_dataset()\n",
    "        self.char_to_int = self.build_vocab()\n",
    "        \n",
    "    def load_dataset(self):\n",
    "        labelled_pairs = []\n",
    "        with open(f\"{self.data_root}/train.txt\", 'r', encoding='utf-8') as file:\n",
    "            file.seek(0)\n",
    "            lines = file.readlines()\n",
    "            for line in lines:\n",
    "                line = line.split()\n",
    "                labelled_pairs.append((line[0], line[1]))\n",
    "        return labelled_pairs\n",
    "    \n",
    "    def build_vocab(self):\n",
    "        unique_chars = []\n",
    "        with open(f\"{self.data_root}/hindi_vocab.txt\", 'r', encoding='utf-8') as file:\n",
    "            file.seek(0)\n",
    "            lines = file.readlines()\n",
    "            for line in lines:\n",
    "                for char in line:\n",
    "                    if char not in unique_chars:\n",
    "                        unique_chars.append(char)\n",
    "        char_to_int = {'<PAD>': 0, '<UNK>': 1}\n",
    "        for i in range(len(unique_chars)):\n",
    "            char_to_int[unique_chars[i]] = i + 2\n",
    "        return char_to_int\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, text = self.samples[idx]\n",
    "        \n",
    "        # load image\n",
    "        img = Image.open(os.path.join(self.data_root, img_path)).convert('L')\n",
    "        if self.transform:\n",
    "            img_tensor = self.transform(img)\n",
    "        \n",
    "        # Process text\n",
    "        text_ids = [self.char_to_int.get(c, self.char_to_int['<UNK>']) for c in text]\n",
    "        padded_text_ids = torch.zeros(self.max_seq_len, dtype=torch.long)\n",
    "        padded_text_ids[:len(text_ids)] = torch.tensor(text_ids)\n",
    "        \n",
    "        return img_tensor, padded_text_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7c73fbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # Use an LSTM to process the sequence of characters\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, text_ids):\n",
    "        # text_ids shape: (Batch_size, Max_seq_len)\n",
    "        embedded = self.embedding(text_ids)\n",
    "        # Pass through RNN/LSTM\n",
    "        _, (hidden, _) = self.rnn(embedded)\n",
    "        # Use the final hidden state and project to the desired output size\n",
    "        # hidden shape: (1, Batch_size, Hidden_size)\n",
    "        condition = self.fc(hidden.squeeze(0))\n",
    "        return condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "50fb8bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, condition_dim, img_channels, img_size_h, img_size_w):\n",
    "        super().__init__()\n",
    "        self.img_size_h = img_size_h\n",
    "        self.img_size_w = img_size_w\n",
    "        self.img_channels = img_channels\n",
    "        \n",
    "        # We start by projecting and reshaping the combined input\n",
    "        # Input: z_dim + condition_dim\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(z_dim + condition_dim, 1024 * (img_size_h // 16) * (img_size_w // 16)),\n",
    "            nn.BatchNorm1d(1024 * (img_size_h // 16) * (img_size_w // 16)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Now, we upsample using ConvTranspose2d\n",
    "        # We'll go from (4x16) -> (8x32) -> (16x64) -> (32x128) -> (64x256)\n",
    "        self.gen = nn.Sequential(\n",
    "            # Input: (1024, 4, 16)\n",
    "            self._block(1024, 512, 4, 2, 1),  # -> (512, 8, 32)\n",
    "            self._block(512, 256, 4, 2, 1),   # -> (256, 16, 64)\n",
    "            self._block(256, 128, 4, 2, 1),   # -> (128, 32, 128)\n",
    "            \n",
    "            # Final layer to get to the target size and channels\n",
    "            nn.ConvTranspose2d(\n",
    "                128, img_channels, kernel_size=4, stride=2, padding=1\n",
    "            ),\n",
    "            # Output: (img_channels, 64, 256)\n",
    "            nn.Tanh() # Normalize output to [-1, 1], matching data normalization\n",
    "        )\n",
    "\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels, out_channels, kernel_size, stride, padding, bias=False\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, z, condition):\n",
    "        # z shape: (N, z_dim)\n",
    "        # condition shape: (N, condition_dim)\n",
    "        \n",
    "        # Combine noise and condition\n",
    "        combined_input = torch.cat([z, condition], dim=1) # (N, z_dim + condition_dim)\n",
    "        \n",
    "        # Project and reshape\n",
    "        x = self.fc(combined_input)\n",
    "        # Reshape to (N, 1024, H/16, W/16) -> (N, 1024, 4, 16)\n",
    "        x = x.view(-1, 1024, self.img_size_h // 16, self.img_size_w // 16)\n",
    "        \n",
    "        # Pass through the generator blocks\n",
    "        return self.gen(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "9f436eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, condition_dim, img_channels, img_size_h, img_size_w):\n",
    "        super().__init__()\n",
    "        \n",
    "        # CNN blocks to process the image\n",
    "        # Input: (img_channels, 64, 256)\n",
    "        self.disc = nn.Sequential(\n",
    "            # -> (128, 32, 128)\n",
    "            self._block(img_channels, 128, 4, 2, 1, use_norm=False), \n",
    "            # -> (256, 16, 64)\n",
    "            self._block(128, 256, 4, 2, 1),\n",
    "            # -> (512, 8, 32)\n",
    "            self._block(256, 512, 4, 2, 1),\n",
    "            # -> (1024, 4, 16)\n",
    "            self._block(512, 1024, 4, 2, 1),\n",
    "        )\n",
    "        \n",
    "        # Flatten and combine with condition\n",
    "        # Output of disc: (N, 1024, 4, 16)\n",
    "        # Flattened size: 1024 * 4 * 16 = 65536\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(1024 * (img_size_h // 16) * (img_size_w // 16) + condition_dim, 1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(1024, 1)\n",
    "            # No Sigmoid here! We'll use BCEWithLogitsLoss for stability.\n",
    "        )\n",
    "\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding, use_norm=True):\n",
    "        layers = [\n",
    "            nn.Conv2d(\n",
    "                in_channels, out_channels, kernel_size, stride, padding, bias=False\n",
    "            )\n",
    "        ]\n",
    "        if use_norm:\n",
    "            layers.append(nn.BatchNorm2d(out_channels))\n",
    "        layers.append(nn.LeakyReLU(0.2))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, condition):\n",
    "        # x shape: (N, C, H, W)\n",
    "        # condition shape: (N, condition_dim)\n",
    "        \n",
    "        x = self.disc(x) # (N, 1024, 4, 16)\n",
    "        \n",
    "        # Flatten and concatenate condition\n",
    "        x_flat = x.view(x.shape[0], -1) # (N, 65536)\n",
    "        combined = torch.cat([x_flat, condition], dim=1) # (N, 65536 + condition_dim)\n",
    "        \n",
    "        # Classify\n",
    "        return self.fc(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "5f900b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchvision.transforms import functional as F\n",
    "\n",
    "class FixedHeightResize:\n",
    "    \"\"\"Resizes an image to a fixed height while preserving the aspect ratio.\"\"\"\n",
    "    def __init__(self, target_height=64):\n",
    "        self.target_height = target_height\n",
    "\n",
    "    def __call__(self, img):\n",
    "        # Get original dimensions (PIL image returns width, height)\n",
    "        original_width, original_height = img.size\n",
    "        # Calculate the new width to maintain the aspect ratio\n",
    "        aspect_ratio = original_width / original_height\n",
    "        new_width = math.ceil(aspect_ratio * self.target_height)\n",
    "        # Resize the image\n",
    "        resized_img = img.resize((new_width, self.target_height), Image.BICUBIC)\n",
    "        return resized_img\n",
    "\n",
    "class PadToWidth:\n",
    "    \"\"\"Pads an image (PIL or Tensor) to a fixed width while maintaining height.\"\"\"\n",
    "    def __init__(self, target_width, fill_color=255): \n",
    "        # For standard handwriting data on a white background, 255 (white) is best.\n",
    "        self.target_width = target_width\n",
    "        self.fill_color = fill_color \n",
    "\n",
    "    def __call__(self, img):\n",
    "        if not isinstance(img, Image.Image):\n",
    "             # Assumes we are operating on a PIL Image before ToTensor()\n",
    "             raise TypeError(\"Input must be a PIL Image.\")\n",
    "\n",
    "        current_width = img.width\n",
    "        \n",
    "        if current_width >= self.target_width:\n",
    "            # If the image is already wide enough (or too wide), we just center-crop it.\n",
    "            # This handles outliers, though you should choose max_width carefully.\n",
    "            return F.center_crop(img, (img.height, self.target_width)) \n",
    "        \n",
    "        # Calculate padding needed (only on the right)\n",
    "        padding_needed = self.target_width - current_width\n",
    "        \n",
    "        # Pad with the background color (left, top, right, bottom)\n",
    "        # We only pad on the right (right padding = padding_needed)\n",
    "        padding = (0, 0, padding_needed, 0) \n",
    "        \n",
    "        return F.pad(img, padding, fill=self.fill_color)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311a858f",
   "metadata": {},
   "source": [
    "### Testing for the time being\n",
    "Move to another notebook once all looks good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "44c000af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu\n",
      "Starting Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                     | 14/69853 [00:52<73:28:06,  3.79s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[162], line 109\u001b[0m\n\u001b[1;32m    107\u001b[0m d_fake_output \u001b[38;5;241m=\u001b[39m disc(fake_images\u001b[38;5;241m.\u001b[39mdetach(), condition\u001b[38;5;241m.\u001b[39mdetach())\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    108\u001b[0m d_fake_loss \u001b[38;5;241m=\u001b[39m criterion(d_fake_output, torch\u001b[38;5;241m.\u001b[39mzeros_like(d_fake_output))\n\u001b[0;32m--> 109\u001b[0m d_fake_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# Total discriminator loss\u001b[39;00m\n\u001b[1;32m    112\u001b[0m d_loss \u001b[38;5;241m=\u001b[39m (d_real_loss \u001b[38;5;241m+\u001b[39m d_fake_loss) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf/lib/python3.11/site-packages/torch/_tensor.py:647\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    639\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    640\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    645\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    646\u001b[0m     )\n\u001b[0;32m--> 647\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    648\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    649\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf/lib/python3.11/site-packages/torch/autograd/__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m _engine_run_backward(\n\u001b[1;32m    355\u001b[0m     tensors,\n\u001b[1;32m    356\u001b[0m     grad_tensors_,\n\u001b[1;32m    357\u001b[0m     retain_graph,\n\u001b[1;32m    358\u001b[0m     create_graph,\n\u001b[1;32m    359\u001b[0m     inputs_tuple,\n\u001b[1;32m    360\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    361\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    362\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf/lib/python3.11/site-packages/torch/autograd/graph.py:829\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    830\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    831\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    833\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torchvision.utils as vutils\n",
    "from tqdm import tqdm\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "    \n",
    "print(f\"Using {DEVICE}\")\n",
    "\n",
    "# Data setup\n",
    "IMG_H = 64\n",
    "IMG_W = 256\n",
    "IMG_CHANNELS = 1\n",
    "MAX_SEQ_LEN = 50\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Model params\n",
    "VOCAB_SIZE = len(dataset.char_to_int)\n",
    "EMBEDDING_DIM = 256\n",
    "CONDITION_DIM = 128 # The output size of TextEncoder\n",
    "Z_DIM = 100 # Noise dimension\n",
    "LR = 2e-4\n",
    "BETA1 = 0.5\n",
    "NUM_EPOCHS = 100 # Add more later\n",
    "\n",
    "data_transforms = T.Compose([\n",
    "    FixedHeightResize(target_height=64),\n",
    "    PadToWidth(target_width=256),\n",
    "    T.Grayscale(num_output_channels=1),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "dataset = HW_Dataset(data_root='IIIT-HW-Hindi_v1', transform=data_transforms)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "# Initialize Models\n",
    "text_encoder = TextEncoder(VOCAB_SIZE, EMBEDDING_DIM, CONDITION_DIM, CONDITION_DIM).to(DEVICE)\n",
    "gen = Generator(Z_DIM, CONDITION_DIM, IMG_CHANNELS, IMG_H, IMG_W).to(DEVICE)\n",
    "disc = Discriminator(CONDITION_DIM, IMG_CHANNELS, IMG_H, IMG_W).to(DEVICE)\n",
    "\n",
    "# Initialize weights\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "gen.apply(weights_init)\n",
    "disc.apply(weights_init)\n",
    "text_encoder.apply(weights_init)\n",
    "\n",
    "# Loss and Optimizers\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "opt_disc = optim.Adam(disc.parameters(), lr=LR, betas=(BETA1, 0.999))\n",
    "opt_gen = optim.Adam(\n",
    "    list(gen.parameters()) + list(text_encoder.parameters()), \n",
    "    lr=LR, \n",
    "    betas=(BETA1, 0.999)\n",
    ")\n",
    "\n",
    "# We'll use a fixed batch of noise and text to see G's progress\n",
    "fixed_noise = torch.randn(BATCH_SIZE, Z_DIM).to(DEVICE)\n",
    "fixed_batch_data = next(iter(train_loader))\n",
    "fixed_real_images, fixed_text_ids = fixed_batch_data\n",
    "fixed_text_ids = fixed_text_ids.to(DEVICE)\n",
    "fixed_real_images = fixed_real_images.to(DEVICE)\n",
    "\n",
    "if not os.path.exists(\"outputs\"):\n",
    "    os.makedirs(\"outputs\")\n",
    "\n",
    "# Save the fixed real batch for comparison\n",
    "vutils.save_image(fixed_real_images, \"outputs/real_samples.png\", normalize=True)\n",
    "\n",
    "print(\"Starting Training...\")\n",
    "\n",
    "progress_bar = tqdm(total=dataset.__len__())\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for batch_idx, (real_images, text_ids) in enumerate(train_loader):\n",
    "        real_images = real_images.to(DEVICE)\n",
    "        text_ids = text_ids.to(DEVICE)\n",
    "        \n",
    "        # Get the condition vector (c)\n",
    "        condition = text_encoder(text_ids)\n",
    "        \n",
    "        # --- Train Discriminator ---\n",
    "        opt_disc.zero_grad()\n",
    "        \n",
    "        # Train with real images\n",
    "        # Use condition.detach() so text encoder is not updated\n",
    "        d_real_output = disc(real_images, condition.detach()).reshape(-1)\n",
    "        d_real_loss = criterion(d_real_output, torch.ones_like(d_real_output))\n",
    "        d_real_loss.backward()\n",
    "        \n",
    "        # Train with fake images\n",
    "        noise = torch.randn(real_images.size(0), Z_DIM).to(DEVICE)\n",
    "        fake_images = gen(noise, condition.detach())\n",
    "        # Use condition.detach() so generator is not updated\n",
    "        d_fake_output = disc(fake_images.detach(), condition.detach()).reshape(-1)\n",
    "        d_fake_loss = criterion(d_fake_output, torch.zeros_like(d_fake_output))\n",
    "        d_fake_loss.backward()\n",
    "        \n",
    "        # Total discriminator loss\n",
    "        d_loss = (d_real_loss + d_fake_loss) / 2\n",
    "        opt_disc.step()\n",
    "        \n",
    "        # --- Train Generator ---\n",
    "        opt_gen.zero_grad()\n",
    "        \n",
    "        # Get fresh condition vector (no detaching so encoder is updated)\n",
    "        condition_gen = text_encoder(text_ids)\n",
    "        \n",
    "        # Generate new fake images\n",
    "        fake_images_gen = gen(noise, condition_gen)\n",
    "        \n",
    "        # See what the discriminator thinks (no detaching)\n",
    "        g_output = disc(fake_images_gen, condition_gen).reshape(-1)\n",
    "        \n",
    "        # Calculate loss (Generator wants discriminator to think they are real)\n",
    "        g_loss = criterion(g_output, torch.ones_like(g_output))\n",
    "        \n",
    "        # Backprop (updates both generatir and text encoder)\n",
    "        g_loss.backward()\n",
    "        opt_gen.step()\n",
    "        \n",
    "        # Logging\n",
    "        if (batch_idx + 1) % 100 == 0:\n",
    "            print(\n",
    "                f\"[Epoch {epoch+1}/{NUM_EPOCHS}] [Batch {batch_idx+1}/{len(train_loader)}] \"\n",
    "                f\"D Loss: {d_loss.item():.4f} | G Loss: {g_loss.item():.4f}\"\n",
    "            )\n",
    "        progress_bar.update(1)\n",
    "            \n",
    "    # Save generated images at the end of each epoch\n",
    "    with torch.no_grad():\n",
    "        fixed_condition = text_encoder(fixed_text_ids)\n",
    "        fake_samples = gen(fixed_noise, fixed_condition)\n",
    "        vutils.save_image(\n",
    "            fake_samples,\n",
    "            f\"outputs/fake_samples_epoch_{epoch+1}.png\",\n",
    "            normalize=True\n",
    "        )\n",
    "\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643ff052",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
